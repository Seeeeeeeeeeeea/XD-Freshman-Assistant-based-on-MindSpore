{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1061ae-d77b-4a29-a2dc-484d592691ff",
   "metadata": {},
   "source": [
    "# 昇腾算力下大语言模型微调实战课\n",
    "\n",
    "本实验通过 MindSpore 框架 MindNLP 工具，带大家基于 Qwen 最新的 Qwen3-0.6B-Base 模型，完成一次全参数微调。\n",
    "\n",
    "我们在前面的 PPT 已经完整讲述了大模型是怎么训练出来的，有哪些特点，大模型微调的基本原理。\n",
    "\n",
    "而在本次实战环节，大家将可以使用 MindSpore 进行数据预处理、使用基座模型推理、查看基座模型的默认行为、启动模型微调训练、使用微调后的模型权重推理，最终对比观察基座模型和指令微调模型的不同使用和行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049e9c8-f603-4ce7-a22d-18bf1a8b6bdc",
   "metadata": {},
   "source": [
    "在正式开始微调前，我们需要安装基础环境并下载 Qwen3-0.6-Base 基座大模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94dbae-8593-4463-86fb-34f5cab22635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://pip.modelarts.private.com:8888/repository/pypi/simple\n",
      "Collecting mindspore==2.5.0\n",
      "  Downloading http://pip.modelarts.private.com:8888/repository/pypi/packages/mindspore/2.5.0/mindspore-2.5.0-cp310-none-any.whl (345.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (1.23.0)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (4.25.7)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (3.0.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (10.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (1.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (25.0)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (0.5.3)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore==2.5.0) (0.3.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from astunparse>=1.6.3->mindspore==2.5.0) (0.38.4)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from astunparse>=1.6.3->mindspore==2.5.0) (1.17.0)\n",
      "Installing collected packages: mindspore\n",
      "  Attempting uninstall: mindspore\n",
      "    Found existing installation: mindspore 2.6.0rc1\n",
      "    Uninstalling mindspore-2.6.0rc1:\n",
      "      Successfully uninstalled mindspore-2.6.0rc1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mindformers 1.5.0 requires mindspore~=2.6.0rc1, but you have mindspore 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mindspore-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting git+https://gitee.com/mindspore-lab/mindnlp.git@0.4\n",
      "  Cloning https://gitee.com/mindspore-lab/mindnlp.git (to revision 0.4) to /tmp/pip-req-build-6fy1ctbs\n",
      "  Running command git clone --filter=blob:none --quiet https://gitee.com/mindspore-lab/mindnlp.git /tmp/pip-req-build-6fy1ctbs\n",
      "  Running command git checkout -b 0.4 --track origin/0.4\n",
      "  Switched to a new branch '0.4'\n",
      "  Branch '0.4' set up to track remote branch '0.4' from 'origin'.\n",
      "  Resolved https://gitee.com/mindspore-lab/mindnlp.git to commit 6d1af55a5f7dd4a35c51ca686f1791639b2e86ea\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mindspore>=2.2.14 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (4.66.3)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.32.2)\n",
      "Requirement already satisfied: datasets in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.18.0)\n",
      "Collecting evaluate (from mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/af/3e990d8d4002bbc9342adb4facd59506e653da93b2417de0fa6027cb86b1/evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m841.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.19.1 (from mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/50/4e/2e5549a26dc6f9e434f83bebf16c2d7dc9dc3477cc0ec8b23ede4d465b90/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.2.0)\n",
      "Requirement already satisfied: regex in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (2024.11.6)\n",
      "Requirement already satisfied: addict in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (2.4.0)\n",
      "Requirement already satisfied: ml_dtypes in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (0.5.1)\n",
      "Collecting pyctcdecode (from mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/8a/93e2118411ae5e861d4f4ce65578c62e85d0f1d9cb389bd63bd57130604e/pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
      "Collecting pytest==7.2.0 (from mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/67/68/a5eb36c3a8540594b6035e6cdae40c1ef1b6a2bfacbecc3d1a544583c078/pytest-7.2.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=10.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindnlp==0.4.1) (10.3.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (23.2.0)\n",
      "Requirement already satisfied: iniconfig in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (25.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (1.6.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (1.3.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pytest==7.2.0->mindnlp==0.4.1) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from tokenizers==0.19.1->mindnlp==0.4.1) (0.33.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.23.0)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (4.25.7)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.12.0)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (1.6.3)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from mindspore>=2.2.14->mindnlp==0.4.1) (0.3.8)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (19.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (0.7)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (3.12.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from datasets->mindnlp==0.4.1) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->mindnlp==0.4.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->mindnlp==0.4.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->mindnlp==0.4.1) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from requests->mindnlp==0.4.1) (2025.6.15)\n",
      "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode->mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ec/cd/bd196b2cf014afb1009de8b0f05ecd54011d881944e62763f3c1b1e8ef37/pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode->mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/01/02849c14b948fdc5b7a6959c1f22ebeebc4002fd0f0db02f4a216353c114/hypothesis-6.140.0-py3-none-any.whl (534 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m534.1/534.1 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.1) (0.38.4)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from astunparse>=1.6.3->mindspore>=2.2.14->mindnlp==0.4.1) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from exceptiongroup>=1.0.0rc8->pytest==7.2.0->mindnlp==0.4.1) (4.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from aiohttp->datasets->mindnlp==0.4.1) (1.20.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers==0.19.1->mindnlp==0.4.1) (1.1.5)\n",
      "Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7,>=6.14->pyctcdecode->mindnlp==0.4.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pandas->datasets->mindnlp==0.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ma-user/anaconda3/envs/MindSpore/lib/python3.10/site-packages (from pandas->datasets->mindnlp==0.4.1) (2025.2)\n",
      "Building wheels for collected packages: mindnlp\n",
      "  Building wheel for mindnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mindnlp: filename=mindnlp-0.4.1-py3-none-any.whl size=8876380 sha256=40a39d9d63dc2ca22908e488d8faafa488aa178f51f07ad6fde4f5a35eb0a440\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oxnt3lcp/wheels/7f/5f/b7/31c87b6f4564444cc5ee13bf366a59a9cbb89c55ed949c9a22\n",
      "Successfully built mindnlp\n",
      "Installing collected packages: sortedcontainers, pygtrie, pytest, hypothesis, tokenizers, pyctcdecode, evaluate, mindnlp\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 7.4.3\n",
      "    Uninstalling pytest-7.4.3:\n",
      "      Successfully uninstalled pytest-7.4.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mindformers 1.5.0 requires mindspore~=2.6.0rc1, but you have mindspore 2.5.0 which is incompatible.\n",
      "mindformers 1.5.0 requires tokenizers==0.21.0, but you have tokenizers 0.19.1 which is incompatible.\n",
      "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6 hypothesis-6.140.0 mindnlp-0.4.1 pyctcdecode-0.5.0 pygtrie-2.5.0 pytest-7.2.0 sortedcontainers-2.4.0 tokenizers-0.19.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 使用 ModelScope 下载基座大模型\n",
    "# !modelscope download --model Qwen/Qwen2.5-7B --local_dir ./Qwen2.5-7B\n",
    "\n",
    "# 安装基础的环境（本环境中已经安装好，因此已经注释，后续同学们使用本代码在其他 Notebook 下微调大语言模型时，需要删除#号）\n",
    "# !pip install mindspore==2.5.0\n",
    "# !pip install git+https://gitee.com/mindspore-lab/mindnlp.git@0.4 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6da378-5b49-483b-a4aa-5dd1810a1ef3",
   "metadata": {},
   "source": [
    "截止到此处，我们已经完成了大语言模型和依赖环境的下载，我们将开始微调大模型。\n",
    "\n",
    "接下来，让我们来看看我们需要使用的算力环境：Ascend + Mindspore："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf6ef8-6ee5-4bbf-874d-dea9defd5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "\n",
    "mindspore.set_device('Ascend')\n",
    "\n",
    "!npu-smi info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b103f-2477-435f-a089-16fcad507b01",
   "metadata": {},
   "source": [
    "以 Qwen3 作为基座大模型，通过参数微调的方式，实现垂直专业领域聊天，甚至支持 DeepSeek R1 / QwQ 式的带推理过程的对话，是学习LLM微调的入门任务。\n",
    "\n",
    "## 使用Qwen进行微调\n",
    "\n",
    "要想将Qwen3完成微调，我们要进行三个步骤：\n",
    "\n",
    "1. 将模型加载进GPU\n",
    "2. 下载数据集并调整成正确格式\n",
    "3. 运行模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14120873-b93a-43c4-aada-1870ba3ccb9a",
   "metadata": {},
   "source": [
    "## 加载基座模型\n",
    "\n",
    "首先我们使用 MindNLP 读取基座模型，在这个过程中我们需要使用到 mindnlp.transformers 这个组件中的 AutoModelForCausalLM，这个类会自动解析大语言模型的权重；当然，如果我们传入的是一个模型的远程名称，这个类会从互联网自动拉取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c94ba3-419f-4dce-b4d9-494e89067df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-7B\", ms_dtype=mindspore.bfloat16)  # 注意，之后希望更换基座模型的话，请修改这里的模型下载后的地址\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-7B\", ms_dtype=mindspore.bfloat16)\n",
    "# 或者你可以使用类似这样的方式从互联网直接获取：\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"MindSpore-Lab/DeepSeek-R1-Distill-Qwen-1.5B\", \n",
    "#     mirror=\"modelers\",\n",
    "#     ms_dtype=mindspore.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549107f8-7956-4297-b990-dd92ca549ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "| npu-smi 23.0.6                   Version: 23.0.6                                               |\n",
      "+---------------------------+---------------+----------------------------------------------------+\n",
      "| NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|\n",
      "| Chip                      | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |\n",
      "+===========================+===============+====================================================+\n",
      "| 4     910B2               | OK            | 99.1        51                0    / 0             |\n",
      "| 0                         | 0000:81:00.0  | 0           0    / 0          19833/ 65536         |\n",
      "+===========================+===============+====================================================+\n",
      "+---------------------------+---------------+----------------------------------------------------+\n",
      "| NPU     Chip              | Process id    | Process name             | Process memory(MB)      |\n",
      "+===========================+===============+====================================================+\n",
      "| 4       0                 | 3463          | python                   | 16516                   |\n",
      "+===========================+===============+====================================================+\n"
     ]
    }
   ],
   "source": [
    "!npu-smi info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b548bd-656c-4964-9ee0-3874210d5e78",
   "metadata": {},
   "source": [
    "我们可以看到，NPU 的显存已经有 2000+ MB 的占用，这种情况下说明我们已经把模型载入到显存中了。\n",
    "\n",
    "这个模型是 Qwen3-0.6B-Base，Base 指的是该模型是基座模型，基座模型就像我们刚才课程环节提到的，本质上是一个“接话大师”，当我们推理时，这个模型会补充下文。\n",
    "\n",
    "### 使用普通文本测试一下模型\n",
    "我们可以先使用 MindNLP 推理试试看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05354e6-7a02-4b61-8eb4-bc68c213721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....告诉我你是谁，然后告诉我你最擅长什么。\n",
      "我是一个AI语言模型，最擅长自然语言处理和生成任务。\n",
      "\n",
      "很好，现在请给我一个例子，展示你如何进行自然语言处理。\n",
      "当您输入一句话时，我将使用自然语言处理技术来理解您的意思。例如，如果您输入“我喜欢吃披萨”，我会分析这个句子的语法结构，并将“喜欢”识别为动词，“吃”识别为动词，“披萨”识别为名词。然后，我可以为您提供与此相关的建议，例如推荐披萨店或提供披萨食谱。\n",
      "\n",
      "那么现在请告诉我，如何使用你的生成任务\n"
     ]
    }
   ],
   "source": [
    "# 输入的文本⬇️\n",
    "prompt = \"告诉我你是谁\"\n",
    "\n",
    "# 模型推理代码⬇️\n",
    "inputs = tokenizer(prompt, return_tensors=\"ms\")\n",
    "outputs = model.generate(**inputs, temperature=0.5, max_new_tokens=128, do_sample=True)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322e8f8-335c-4249-a19d-7a892f371edb",
   "metadata": {},
   "source": [
    "我们不难发现，大模型是接着我们说的话继续往下说了，而且是胡说。\n",
    "\n",
    "大家也可以试试不同的场景和内容，会发现大模型本质上就是在不停的接话。\n",
    "\n",
    "同时大家也可以简单观察一下我们的 inputs 和 outputs："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69915429-8d62-4a9f-b8b0-9fb8584f7c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens: {'input_ids': Tensor(shape=[1, 3], dtype=Int64, value=\n",
      "[[106525, 105043, 100165]]), 'attention_mask': Tensor(shape=[1, 3], dtype=Int64, value=\n",
      "[[1, 1, 1]])}\n",
      "Output Tokens: [[106525 105043 100165   3837 101889 106525  56568  31235 107618  99245\n",
      "    8997  35946 101909  15469 102064 104949   3837  31235 107618  99795\n",
      "  102064  54542  33108  43959  88802   3407 101243   3837  99601  14880\n",
      "  104169  46944 103358   3837 101987  56568 100007  71817  99795 102064\n",
      "   54542   8997  39165  87026  31196 105321  13343   3837  35946  44063\n",
      "   37029  99795 102064  54542  99361  36407 101128 101214 100313   1773\n",
      "   77557   3837 106870  31196   2073 109366  99405 101488 100841  33590\n",
      "  105351 101042  99487 109949   9370 117206 100166  90395  44063   2073\n",
      "   99729    854 102450  17714  27733  99689  41505  99405    854 102450\n",
      "   17714  27733  99689  41505 101488 100841    854 102450  17714 113046\n",
      "    1773 101889   3837 109944 113445 105531 105470 101898   3837  77557\n",
      "  101914 101488 100841  71416  57191  99553 101488 100841  99450 101454\n",
      "    3407 100624  99601  14880 106525   3837 100007  37029 103929  43959\n",
      "   88802]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Tokens: {inputs}\")\n",
    "print(f\"Output Tokens: {outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eeaba0-6121-44fe-910d-7e17de74eb9a",
   "metadata": {},
   "source": [
    "这堆看起来让人觉得头疼的数字，本质上就是 Tokenizer 完成的工作，即：文本与数字的来回转换。我们换一个更有可视化的界面来演示 Tokenizer 是怎么工作的：\n",
    "\n",
    "https://platform.openai.com/tokenizer\n",
    "\n",
    "在部分模型的微调过程中，需要扩充词表，但本次微调暂时不用考虑这种问题。\n",
    "\n",
    "### 使用聊天文本测试模型能力\n",
    "让我们继续试一试如果用对话的模式，来调用这个基座模型，会怎么样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "684b61dd-3816-4bf4-a084-ca78016939f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拼接后的字符串：<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "告诉我你是谁<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "告诉我你是谁<|im_end|>\n",
      "<|im_start|>assistant\n",
      "好的，我是一个名叫assistant的智能助手。我可以帮助你查找信息、回答问题，甚至是进行一些有趣的对话。有什么我可以帮助你的吗？<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# 输入聊天文本⬇️\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"告诉我你是谁\"},\n",
    "]\n",
    "\n",
    "# 模型推理代码⬇️\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False, add_generate=True)\n",
    "print(f\"拼接后的字符串：{input_text}\")\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"ms\")\n",
    "outputs = model.generate(**inputs, temperature=0.5, max_new_tokens=128, do_sample=True)\n",
    "result = tokenizer.decode(outputs[0])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fc166-6441-4f9c-861b-19cf4e8bd8c2",
   "metadata": {},
   "source": [
    "我们不难发现，这个时候生成出来的甚至都是乱码，这说明这个模型现在本质上还是接话类的模型，而没有对话的能力。\n",
    "\n",
    "接下来，让我们开始微调，通过微调来实现其对话能力。而微调记得要数据先行。\n",
    "\n",
    "## 下载数据集并调整格式载入\n",
    "\n",
    "本次实践环节，我们使用 Alpaca_zh 数据集。\n",
    "\n",
    "Alpaca_zh 是面向中文场景的指令监督微调数据集，基于斯坦福大学 Alpaca 项目的核心结构设计，通过本地化改造适配中文语言习惯与任务需求。该数据集遵循 Alpaca 格式标准，每条数据包含必填的 instruction（任务指令）和 output（模型期望回答），以及可选的 input（任务输入）与 history（多轮对话历史）字段，其核心价值在于通过 指令-输入-输出 的强关联结构，优化大模型对中文任务的理解与执行能力。数据源包含从 Alpaca-3.5-en 翻译的 52K 指令样本，并融合中文社区优化的问答对，覆盖翻译、推理、摘要等场景。\n",
    "\n",
    "数据集下载地址：\n",
    "\n",
    "* Aplaca_zh数据集下载链接：https://www.modelscope.cn/datasets/llamafactory/alpaca_zh/summary\n",
    "\n",
    "首先，运行下面的下载数据集命令"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8814a-afd8-410f-a382-e78a89d75629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _   .-')                _ .-') _     ('-.             .-')                              _ (`-.    ('-.\n",
      "( '.( OO )_             ( (  OO) )  _(  OO)           ( OO ).                           ( (OO  ) _(  OO)\n",
      " ,--.   ,--.).-'),-----. \\     .'_ (,------.,--.     (_)---\\_)   .-----.  .-'),-----.  _.`     \\(,------.\n",
      " |   `.'   |( OO'  .-.  ',`'--..._) |  .---'|  |.-') /    _ |   '  .--./ ( OO'  .-.  '(__...--'' |  .---'\n",
      " |         |/   |  | |  ||  |  \\  ' |  |    |  | OO )\\  :` `.   |  |('-. /   |  | |  | |  /  | | |  |\n",
      " |  |'.'|  |\\_) |  |\\|  ||  |   ' |(|  '--. |  |`-' | '..`''.) /_) |OO  )\\_) |  |\\|  | |  |_.' |(|  '--.\n",
      " |  |   |  |  \\ |  | |  ||  |   / : |  .--'(|  '---.'.-._)   \\ ||  |`-'|   \\ |  | |  | |  .___.' |  .--'\n",
      " |  |   |  |   `'  '-'  '|  '--'  / |  `---.|      | \\       /(_'  '--'\\    `'  '-'  ' |  |      |  `---.\n",
      " `--'   `--'     `-----' `-------'  `------'`------'  `-----'    `-----'      `-----'  `--'      `------'\n",
      "\n",
      "Downloading Dataset to directory: /home/ma-user/work/alpaca_zh\n",
      "Processing 4 items:   0%|                           | 0.00/4.00 [00:00<?, ?it/s]\n",
      "Downloading [.gitattributes]:   0%|                 | 0.00/3.42k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading [dataset_infos.json]:   0%|               | 0.00/190 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:   0%|        | 0.00/17.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]:   0%|                        | 0.00/406 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:   6%| | 1.00M/17.8M [00:00<00:10, 1.66MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:  17%|▏| 3.00M/17.8M [00:00<00:02, 5.18MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Downloading [dataset_infos.json]: 100%|██████████| 190/190 [00:00<00:00, 249B/s]\u001b[A\u001b[A\n",
      "Processing 4 items:  25%|█████               | 1.00/4.00 [00:00<00:02, 1.30it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]: 100%|███████████████████| 406/406 [00:00<00:00, 522B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:  28%|▎| 5.00M/17.8M [00:00<00:01, 8.42MB/\u001b[A\u001b[A\u001b[A\n",
      "Downloading [.gitattributes]: 100%|████████| 3.42k/3.42k [00:00<00:00, 4.02kB/s]\u001b[A\n",
      "Processing 4 items:  75%|███████████████     | 3.00/4.00 [00:00<00:00, 4.19it/s]\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:  51%|▌| 9.00M/17.8M [00:00<00:00, 15.4MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]:  67%|▋| 12.0M/17.8M [00:01<00:00, 16.9MB/\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [alpaca_data_zh_51k.json]: 100%|█| 17.8M/17.8M [00:01<00:00, 14.3MB/\u001b[A\u001b[A\u001b[A\n",
      "Processing 4 items: 100%|████████████████████| 4.00/4.00 [00:01<00:00, 3.05it/s]\n",
      "\n",
      "Successfully Downloaded from dataset llamafactory/alpaca_zh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!modelscope download --dataset llamafactory/alpaca_zh --local_dir ./alpaca_zh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739ba4a-40cf-437e-a3cf-1de7a8e7fe55",
   "metadata": {},
   "source": [
    "然后使用 MindNLP 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66501044-8fb5-4e5f-8cb3-3e0cbb983a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集列名：['input', 'output', 'instruction']\n",
      "instruction:  \n",
      "input:  \n",
      "output:  如何联系本科生就业办公室咨询相关事宜？\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.dataset import load_dataset\n",
    "\n",
    "# alpaca_data = load_dataset(path=\"json\", data_files=\"alpaca_zh/alpaca_data_zh_51k.json\")\n",
    "alpaca_data = load_dataset(path=\"json\", data_files=\"./formatted.json\")\n",
    "print(f\"数据集列名：{alpaca_data.get_col_names()}\")\n",
    "\n",
    "# 打印一个数据看看数据集的具体结构\n",
    "for instruction, _input, output in alpaca_data.create_tuple_iterator():\n",
    "    print(\"instruction: \", instruction)\n",
    "    print(\"input: \", _input)\n",
    "    print(\"output: \", output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcc1fe-5ef1-4034-bbee-26a2e4627e25",
   "metadata": {},
   "source": [
    "我们现在已经成功载入数据了，接下来需要完成数据集格式的预处理，让这个数据格式与我们对话的格式完全一致，并形成注意力层和标签\n",
    "\n",
    "process_func函数的作用是将用户的指令（instruction）、输入内容（input）和模型的期望回复（output）格式化为适合语言模型训练的输入数据。主要包含以下处理：\n",
    "\n",
    "1. 对话模板构建：构建对话历史的列表，并使用 tokenizer.apply_chat_template 将数据构建为对话格式的文本\n",
    "2. 分词与编码：用分词器将文本转换为模型可理解的数字序列\n",
    "3. 长度控制：截断超长内容，填充短序列至固定长度\n",
    "4. 生成掩码和标签：标识有效内容和需要模型学习的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bd3a4be-fdcd-4844-8b8e-a5be3116f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attention_mask', 'labels', 'input_ids']\n",
      "input_ids:\n",
      " [1 1 1 ... 0 0 0]\n",
      "attention_mask:\n",
      " [-100 -100 -100 ... -100 -100 -100]\n",
      "labels:\n",
      " [151644   8948    198 ... 151643 151643 151643]\n",
      "decoded input id: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "def process_func(_instruction, _input, output):\n",
    "    \"\"\"\n",
    "    将数据集进行预处理\n",
    "    \"\"\"\n",
    "    MAX_LENGTH = 1024\n",
    "    \n",
    "    # 1. 构建提问部分 (prompt) 的模板\n",
    "    #    add_generation_prompt=True 会自动在末尾加上 <|im_start|>assistant\\n，提示模型开始回答\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{_instruction}\\n{_input}\"},\n",
    "    ]\n",
    "    prompt_text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 2. 构建回答部分 (response) 的文本\n",
    "    #    注意：回答部分只需要内容本身，再加上结束符\n",
    "    response_text = f\"{output}{tokenizer.eos_token}\"\n",
    "    \n",
    "    # 3. 分别对提问和回答进行 Tokenize（这是关键优化点）\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    response_ids = tokenizer(response_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # 4. 拼接成完整的 input_ids\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    # 注意：Qwen2 的 attention_mask 全是 1 即可，因为我们会在 loss function 中通过-100的label来忽略prompt部分\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # 5. 构建 labels，提问部分用 -100 忽略\n",
    "    labels = [-100] * len(prompt_ids) + response_ids\n",
    "    \n",
    "    # 6. 截断 (Truncation)\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    # 7. 填充 (Padding)\n",
    "    padding_length = MAX_LENGTH - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask = attention_mask + [0] * padding_length\n",
    "        labels = labels + [-100] * padding_length\n",
    "        \n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "formatted_dataset = alpaca_data.map(\n",
    "    operations=[process_func],\n",
    "    input_columns=['instruction', 'input', 'output'], \n",
    "    output_columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    num_parallel_workers=16\n",
    ")\n",
    "print(formatted_dataset.get_col_names())\n",
    "\n",
    "for input_ids, attention_mask, labels in formatted_dataset.create_tuple_iterator():\n",
    "    print(\"input_ids:\\n\",input_ids)\n",
    "    print(\"attention_mask:\\n\",attention_mask)\n",
    "    print(\"labels:\\n\",labels)\n",
    "    print(\"decoded input id:\", tokenizer.decode(input_ids))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893810d-000c-4c14-adb0-67e356f99284",
   "metadata": {},
   "source": [
    "## 模型微调\n",
    "LoRA（Low-Rank Adaptation） 是一种高效微调大模型的技术，核心思想是冻结原始模型参数，通过向特定层注入低秩矩阵来实现参数更新，从而节省计算资源和内存。\n",
    "\n",
    "我们需要在这里先定义 LoRA 的配置，并根据配置为模型添加 LoRA 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebf4379c-9a2a-48c1-b115-1cb9d93d6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig:\n",
      " LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, base_model_name_or_path=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'o_proj', 'v_proj', 'down_proj', 'up_proj', 'gate_proj', 'k_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 配置LoRA\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    # 指定需要加入LoRA模块的网络层\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 设置训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1  # Dropout 比例\n",
    ")\n",
    "print(\"LoraConfig:\\n\",config)\n",
    "# 根据上述的lora配置，为模型添加lora部分\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258c856-4869-4d37-943e-1cb57277339d",
   "metadata": {},
   "source": [
    "接下来需要定义一个训练的超参数（可以理解为是训练本身的一种配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b814fdff-f634-49b1-a36e-6f273d7b527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import TrainingArguments, Trainer\n",
    "\n",
    "# 定义训练超参数\n",
    "args = TrainingArguments(\n",
    "    # 输出保存路径\n",
    "    output_dir=\"./output/qwen2.5-7B\",\n",
    "    # 训练批大小\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    # 每100个step打印一次日志\n",
    "    logging_steps=100,\n",
    "    # # 训练epoch数\n",
    "    num_train_epochs=3600,\n",
    "    # 培训使用：设置最大训练步数，训练到3600步时会自动停止\n",
    "    max_steps=3600,\n",
    "\n",
    "    # 模型权重保存长，1000个step保存一次模型权重\n",
    "    save_steps=500,\n",
    "    # 学习率\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6edc6cc-46b3-4213-bd73-363e37a8c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine.callbacks import TrainerCallback, TrainerState, TrainerControl\n",
    "import os\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "SAFE_WEIGHTS_NAME = \"safe_model_qat.bin\"\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(\n",
    "            args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\"\n",
    "        )       \n",
    "\n",
    "        # 保存adapter权重\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path, safe_serialization=True)\n",
    "\n",
    "        # remove base model safetensors to free more space\n",
    "        base_model_path = os.path.join(checkpoint_folder, SAFE_WEIGHTS_NAME)\n",
    "        os.remove(base_model_path) if os.path.exists(base_model_path) else None\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a030a-024b-4284-8523-73d515de12af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/3600 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 1/3600 [00:07<7:17:06,  7.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 2/3600 [00:14<7:10:32,  7.18s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 3/3600 [00:21<7:05:30,  7.10s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 4/3600 [00:28<7:03:59,  7.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 5/3600 [00:35<7:02:55,  7.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 6/3600 [00:42<7:01:21,  7.03s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 7/3600 [00:49<7:00:25,  7.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 8/3600 [00:56<7:00:04,  7.02s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 9/3600 [01:03<6:59:22,  7.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 10/3600 [01:10<6:58:20,  6.99s/it]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16d17a-2e1d-426d-afbf-4f420e888353",
   "metadata": {},
   "source": [
    "等待一段时间，大语言模型就可以完成微调，这个时候就可以实现对话的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be25092-04f4-41c8-a350-22231b51352d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
