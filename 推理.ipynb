{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9612fde-f46f-4818-a7d0-f291bc3a5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:39.782.042 [mindspore/run_check/_check_version.py:324] MindSpore version 2.5.0 and Ascend AI software package (Ascend Data Center Solution)version 7.7 does not match, the version of software package expect one of ['7.5', '7.6']. Please refer to the match info on: https://www.mindspore.cn/install\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:39.784.697 [mindspore/run_check/_check_version.py:402] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:39.785.480 [mindspore/run_check/_check_version.py:342] MindSpore version 2.5.0 and \"te\" wheel package version 7.7 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:39.786.258 [mindspore/run_check/_check_version.py:349] MindSpore version 2.5.0 and \"hccl\" wheel package version 7.7 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:39.787.013 [mindspore/run_check/_check_version.py:363] Please pay attention to the above warning, countdown: 3\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:40.788.510 [mindspore/run_check/_check_version.py:363] Please pay attention to the above warning, countdown: 2\n",
      "[WARNING] ME(16575:281472988614832,MainProcess):2025-09-22-18:46:41.790.741 [mindspore/run_check/_check_version.py:363] Please pay attention to the above warning, countdown: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+\n",
      "| npu-smi 23.0.6                   Version: 23.0.6                                               |\n",
      "+---------------------------+---------------+----------------------------------------------------+\n",
      "| NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|\n",
      "| Chip                      | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |\n",
      "+===========================+===============+====================================================+\n",
      "| 0     910B2               | OK            | 94.4        49                0    / 0             |\n",
      "| 0                         | 0000:C1:00.0  | 0           0    / 0          3379 / 65536         |\n",
      "+===========================+===============+====================================================+\n",
      "+---------------------------+---------------+----------------------------------------------------+\n",
      "| NPU     Chip              | Process id    | Process name             | Process memory(MB)      |\n",
      "+===========================+===============+====================================================+\n",
      "| No running processes found in NPU 0                                                            |\n",
      "+===========================+===============+====================================================+\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from mindnlp.peft import PeftModel\n",
    "\n",
    "# 设置设备（Ascend/GPU/CPU）\n",
    "mindspore.set_device('Ascend')\n",
    "!npu-smi info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12cb92-d082-4ff6-898a-0d316e60320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`.`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 加载基座模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(16575,fffe933b91e0,python):2025-09-22-18:51:30.570.788 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_vmm_adapter.h:147] CheckVmmDriverVersion] Open file /etc/ascend_install.info failed.\n",
      "[WARNING] DEVICE(16575,fffe933b91e0,python):2025-09-22-18:51:30.570.994 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_vmm_adapter.h:186] CheckVmmDriverVersion] Driver version is less than 24.0.0, vmm is disabled by default, drvier_version: 23.0.6\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 加载 tokenizer...\n",
      ">>> 加载 LoRA adapter 权重...\n"
     ]
    }
   ],
   "source": [
    "# ========= 1. 加载基座模型 =========\n",
    "base_model_path = \"./Qwen2.5-7B\"   # 你的基座模型路径\n",
    "adapter_model_path = \"./output/qwen2.5-7B/checkpoint-25/adapter_model\"  # LoRA 权重路径\n",
    "\n",
    "print(\">>> 加载基座模型...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    ms_dtype=mindspore.bfloat16\n",
    ")\n",
    "\n",
    "print(\">>> 加载 tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# ========= 2. 加载 LoRA 微调权重 =========\n",
    "print(\">>> 加载 LoRA adapter 权重...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43405ff8-60a0-4ea3-ac3e-fca4b1ebb82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "....\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "如何联系本科生就业办公室咨询相关事宜？\n",
      "assistant\n",
      "81891632本科生就业办公室\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"如何联系本科生就业办公室咨询相关事宜？\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5bda2-f415-4390-9055-bc6b236b9c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "校园乐跑的有效跑步标准是什么？\n",
      "assistant\n",
      "男生有效跑步标准：单次跑步里程达到3km，配速在3～9min/km范围内。  \n",
      "女生有效跑步标准：单次跑步里程达到2km，配速在3～9min/km范围内。  \n",
      "（每天6:00～23:00完成，体育课内跑步无效）\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"校园乐跑的有效跑步标准是什么？\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca311a3-f6f5-4eb6-bcca-4d01317cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "新生入学时是否需要参加开学考试？\n",
      "assistant\n",
      "根据校规内容，新生入学时需要参加开学考试。具体考试安排和要求请以学校正式通知为准。\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"新生入学时是否需要参加开学考试？\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68d7ab-0ed6-469b-9983-5c3e6031a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "如果我想查看奖学金、助学金的相关通知，应该关注哪个公众号？\n",
      "assistant\n",
      "西电学生奖助\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"我校的校训是什么\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83ff69-9ad2-4620-bb95-3042b7bd1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "学校有几个菜鸟驿站，分别在哪里?\n",
      "assistant\n",
      "学校有以下4个菜鸟驿站：\n",
      "\n",
      "1. 菜鸟驿站（老综合楼靠北门边）\n",
      "2. 菜鸟驿站（老综合楼靠南门边）\n",
      "3. 菜鸟驿站（老综合楼香八度二楼）\n",
      "4. 菜鸟驿站（丁香12号楼后）\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"学校有几个菜鸟驿站，分别在哪里?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b00f135-38f3-4cc7-a6b9-aa55ff47ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "西安电子科技大学南校区地址在哪?\n",
      "assistant\n",
      "西安市太白南路2号\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"西安电子科技大学南校区地址在哪?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaefce6-fb61-42a7-8519-5761378b8f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "开学考的成绩是否会影响实验班的分流？?\n",
      "assistant\n",
      "是的，开学考的成绩会影响实验班的分流。如果你高考直接进入了实验班，开学考的成绩将决定你的班级分流。实验班相当于一次转专业的机会，想转专业的同学可以关注开学考。更多详情可以参考：https://mobile.xidian.edu.cn/info/1228/57686.htm\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"开学考的成绩是否会影响实验班的分流？?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ad0fe-c94e-4b2c-8afc-1236aad5f370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "在哪里可以找到京东快递点？?\n",
      "assistant\n",
      "京东快递点位于老综合楼靠北门边缘。\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"在哪里可以找到京东快递点？?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f15ad6-b5a9-4247-aec6-e1fdeaaf50ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "我校的校训是什么?\n",
      "assistant\n",
      "厚德、求真、砺学、笃行\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"我校的校训是什么?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f54a6-9933-4517-9be2-c54d3239d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "考试作弊会面临什么处分?\n",
      "assistant\n",
      "根据学生手册规定，考试作弊将视情节轻重给予记过及以上处分。具体包括以下行为：\n",
      "1. 代替他人或让他人代替自己参加考试\n",
      "2. 组织作弊\n",
      "3. 使用通讯设备或其他器材作弊\n",
      "4. 向他人出售考试试题或答案牟取利益\n",
      "5. 其他严重作弊行为\n",
      "6. 扰乱考场秩序\n",
      "7. 故意扰乱考场秩序\n",
      "8. 抢夺、窃取试卷或答案\n",
      "9. 故意销毁试卷、答卷或考试材料\n",
      "10. 拒绝、妨碍考试工作人员履行管理职责\n",
      "11. 威胁、侮辱、诽谤、诬陷考试工作人员\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"考试作弊会面临什么处分?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87311aa3-6a52-4081-a5e3-f788782797a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Both `max_new_tokens` (=2048) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 开始生成...\n",
      "\n",
      "===== 模型输出 =====\n",
      "\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "学校本科生宿舍的基本配置有哪些?\n",
      "assistant\n",
      "本科生宿舍基本配置包括：上床下桌，配备电话、暖气、空调和网络，提供热水洗浴服务。宿舍区域分布在竹园、海棠、丁香等公寓，各区域均设有学生餐厅。\n"
     ]
    }
   ],
   "source": [
    "# ========= 3. 构造输入 =========\n",
    "prompt_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"学校本科生宿舍的基本配置有哪些?\"}\n",
    "]\n",
    "\n",
    "# 转换成模型输入格式\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    prompt_messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"ms\")\n",
    "\n",
    "# ========= 4. 推理生成 =========\n",
    "print(\">>> 开始生成...\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=512,\n",
    "    do_sample=True,    # 采样模式（更有创意）\n",
    "    top_p=0.9,         # nucleus sampling\n",
    "    temperature=0.8    # 控制多样性\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n===== 模型输出 =====\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47c4d3-661c-478f-bd7b-e0cb06d49ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
